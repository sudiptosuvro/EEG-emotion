{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fcbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io import loadmat\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2143bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ed84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 200\n",
    "# Choose bands\n",
    "\"\"\" \n",
    "band_dict = {\n",
    "    \"delta\": [1, 4],\n",
    "    \"theta\": [4, 8],\n",
    "    \"alpha\": [8, 14],\n",
    "    \"beta\": [14, 31],\n",
    "    \"gamma\": [31, 50],\n",
    "} \n",
    "\"\"\"\n",
    "band_dict = {\n",
    "    \"delta\": [1, 4],\n",
    "    \"theta\": [4, 8],\n",
    "    \"alpha\": [8, 14],\n",
    "    \"beta\": [14, 31],\n",
    "    \"gamma\": [31, 50],\n",
    "}\n",
    "\n",
    "emotion_lable_dict = {\n",
    "    -1: \"negative\",\n",
    "    0: \"neutral\",\n",
    "    1: \"positive\"\n",
    "}\n",
    "\n",
    "# F7, F8, T7, T8 in SEED data set\n",
    "channels = [5, 13, 23, 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315881f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier:\n",
    "    \"\"\"This is a class for emotion recognition classifier\n",
    "\n",
    "    This class contains the reading of the dataset seed,\n",
    "    the feature extraction of the data, the training and testing of the several model,\n",
    "    and finding the optimal parameters of the several model\n",
    "\n",
    "    Attributes:\n",
    "        data_dir: The path of SEED dataset directory.\n",
    "        feature_data_dir: The path of feature data directory.\n",
    "    \"\"\"\n",
    "\n",
    "    datasets_X, datasets_y = [], []\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "    y_pred = []\n",
    "    SVM_params = {\"C\": 0.1, \"kernel\": \"linear\"}\n",
    "    AdaBoost_params = {\"n_estimators\": 2000, \"learning_rate\": 0.01}\n",
    "    flag = False\n",
    "    usr_data_path = []\n",
    "\n",
    "    MLP_params = {\n",
    "        \"activation\": \"tanh\",\n",
    "        \"alpha\": 0.05,\n",
    "        \"hidden_layer_sizes\": (500, 3),\n",
    "        \"learning_rate\": \"adaptive\",\n",
    "        \"max_iter\": 1400,\n",
    "        \"solver\": \"sgd\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc51c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    " def __init__(\n",
    "        self,\n",
    "        flag=False, \n",
    "        data_dir=\"../SEED/Preprocessed_EEG/\", \n",
    "        feature_data_dir=\"../TrainingData/\", \n",
    "        usr_data_path=\"../TestData/BrainFlow-RAW.csv\"\n",
    "    ):\n",
    "        \"\"\"Inits EmotionClassifier Class\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): The path of SEED dataset directory.\n",
    "            feature_data_dir (str): The path of featured data directory.\n",
    "        \"\"\"\n",
    "        self.usr_data_path = usr_data_path\n",
    "        if not EmotionClassifier.__data_exist(feature_data_dir):\n",
    "            print(\"/*********//* Feature data does not exit *//**********/\")\n",
    "            print(\"/****************//* creating data *//****************/\")\n",
    "            self.feature_extraction(data_dir, feature_data_dir)\n",
    "        else:\n",
    "            print(\"/*************//* Feature data exist *//**************/\")\n",
    "            print(\"/****************//* reading data *//*****************/\")\n",
    "            self.__feature_data_io(feature_data_dir, \"rb\")\n",
    "        self.flag = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6e2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(self, data_dir, feature_data_dir):\n",
    "        \"\"\"Read the data, perform bandpass filtering on the data,\n",
    "        and calculate the DE of the data\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): The path of SEED dataset directory.\n",
    "            feature_data_dir (str): The path of featured data directory.\n",
    "        \"\"\"\n",
    "        label_Y = loadmat(data_dir + \"label.mat\")[\"label\"][0]\n",
    "        file_count = 0\n",
    "        for file_name in os.listdir(data_dir):\n",
    "            if file_name in [\"label.mat\", \"readme.txt\"]:\n",
    "                continue\n",
    "            file_data = loadmat(data_dir + file_name)\n",
    "            file_count += 1\n",
    "            print(\n",
    "                \"Currently processed to: {}ï¼Œtotal progress: {}/{}\".format(\n",
    "                    file_name, file_count, len(os.listdir(data_dir)) - 2\n",
    "                )\n",
    "            )\n",
    "            label_data = list(file_data.keys())[3:]\n",
    "            for index, lable in enumerate(label_data):\n",
    "                data = file_data[lable][channels]\n",
    "                self.datasets_X.append(self.process_data(data, fs, channels))\n",
    "                self.datasets_y.append(label_Y[index])\n",
    "\n",
    "        self.datasets_X = np.array(self.datasets_X)\n",
    "        self.datasets_y = self.datasets_y\n",
    "        self.__feature_data_io(feature_data_dir, \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e43ac3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    " def process_data(self, data, fs, channels):\n",
    "        dataset_X = []\n",
    "        for band in band_dict.values():\n",
    "            b, a = signal.butter(4, [band[0] / fs, band[1] / fs], \"bandpass\")\n",
    "            filtedData = signal.filtfilt(b, a, data)\n",
    "            filtedData_de = []\n",
    "            for channel in range(len(channels)):\n",
    "                filtedData_split = []\n",
    "                for de_index in range(0, filtedData.shape[1] - fs, fs):\n",
    "                    # Calculate DE\n",
    "                    filtedData_split.append(\n",
    "                        math.log(\n",
    "                            2\n",
    "                            * math.pi\n",
    "                            * math.e\n",
    "                            * np.var(\n",
    "                                filtedData[channel, de_index: de_index + fs],\n",
    "                                ddof=1,\n",
    "                            )\n",
    "                        )\n",
    "                        / 2\n",
    "                    )\n",
    "                filtedData_split = filtedData_split[-100:]\n",
    "                filtedData_de.append(filtedData_split)\n",
    "            filtedData_de = np.array(filtedData_de)\n",
    "            dataset_X.append(filtedData_de)\n",
    "        dataset_X = np.array(dataset_X).reshape(\n",
    "            (len(channels) * 100 * len(band_dict.keys()))\n",
    "        )  # channels_num * 100 * band_num\n",
    "        # self.print_wave(dataset_X)\n",
    "        return dataset_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d3992b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __feature_data_io(self, feature_data_dir, method):\n",
    "        \"\"\"IO functions to read or write feature data\n",
    "\n",
    "        Args:\n",
    "            feature_data_dir (str): The path of featured data directory.\n",
    "            method (str): read -- \"rb\" or write -- \"wb\"\n",
    "        \"\"\"\n",
    "        with open(feature_data_dir + \"datasets_X.pickle\", method) as fx:\n",
    "            if method == \"rb\":\n",
    "                self.datasets_X = pickle.load(fx)\n",
    "            else:\n",
    "                pickle.dump(self.datasets_X, fx)\n",
    "        with open(feature_data_dir + \"datasets_y.pickle\", method) as fy:\n",
    "            if method == \"rb\":\n",
    "                self.datasets_y = pickle.load(fy)\n",
    "            else:\n",
    "                pickle.dump(self.datasets_y, fy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00602287",
   "metadata": {},
   "outputs": [],
   "source": [
    " def __data_exist(path):\n",
    "        \"\"\"Determine if the folder where the path is located exists or is empty\n",
    "\n",
    "        Note:\n",
    "            If the folder does not exist, create the folder.\n",
    "            Return false if the folder does not exist or is empty\n",
    "            Returns true if the folder exists and is not empty\n",
    "        Args:\n",
    "            path (str): The path of giving directory.\n",
    "        Returns: Boolean\n",
    "        \"\"\"\n",
    "        isExists = os.path.exists(path)\n",
    "        if not isExists:\n",
    "            os.makedirs(path)\n",
    "            return False\n",
    "        else:\n",
    "            if os.path.getsize(path) < 100:\n",
    "                return False\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de0e87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    " def Init_train_test_data(self, test_size=0.2):\n",
    "        \"\"\"Initialize training data and test data\n",
    "\n",
    "        Args:\n",
    "            test_size : float or int, default=0.2\n",
    "            If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "            of the dataset to include in the test split. If int, represents the\n",
    "            absolute number of test samples.\n",
    "        \"\"\"\n",
    "        print(\"/*********//* Initializing training data *//**********/\")\n",
    "        if self.flag:\n",
    "            self.X_train = self.y_train = self.datasets_X\n",
    "            self.X_test = self.y_test = self.datasets_y\n",
    "        else:\n",
    "            self.X_train, self.y_train, self.X_test, self.y_test = train_test_split(\n",
    "                self.datasets_X, self.datasets_y, test_size=test_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0ca23",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cafea336",
   "metadata": {},
   "outputs": [],
   "source": [
    " def SVM_model(self, find_params=False):\n",
    "        \"\"\"Set and train SVM model, and output the summary.\n",
    "\n",
    "        Args:\n",
    "            find_params : boolean, default=False\n",
    "            If true, do find best parameters for SVM model\n",
    "        \"\"\"\n",
    "        if find_params:\n",
    "            self.model_find_best_params(\"SVM\")\n",
    "        self.model_train(\"SVM\")\n",
    "        if not self.flag:\n",
    "            self.model_summary(\"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6badd5",
   "metadata": {},
   "source": [
    "# AdaBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48cebd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost_model(self, find_params=False):\n",
    "        \"\"\"Set and train AdaBoost model, and output the summary.\n",
    "\n",
    "        Args:\n",
    "            find_params : boolean, default=False\n",
    "            If true, do find best parameters for AdaBoost model\n",
    "        \"\"\"\n",
    "        if find_params:\n",
    "            self.model_find_best_params(\"Ada\")\n",
    "        self.model_train(\"Ada\")\n",
    "        if not self.flag:\n",
    "            self.model_summary(\"Ada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cd260",
   "metadata": {},
   "source": [
    "# MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a070c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model(self, find_params=False):\n",
    "        \"\"\"Set and train MLP model, and output the summary.\n",
    "\n",
    "        Args:\n",
    "            find_params : boolean, default=False\n",
    "            If true, do find best parameters for MLP model\n",
    "        \"\"\"\n",
    "        if find_params:\n",
    "            self.model_find_best_params(\"MLP\")\n",
    "        self.model_train(\"MLP\")\n",
    "        if not self.flag:\n",
    "            self.model_summary(\"MLP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
